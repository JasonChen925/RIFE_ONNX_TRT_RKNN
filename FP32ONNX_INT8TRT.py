import tensorrt as trt
import pycuda.driver as cuda
import pycuda.autoinit

import os
import numpy as np

import os
import cv2
import torch
import numpy as np
from glob import glob

# è®¾ç½®æ•°æ®é›†è·¯å¾„ï¼ˆå¤šä¸ªå­æ–‡ä»¶å¤¹ï¼‰
dataset_root = r"/home/jason/RIFE_ONNX_TRT_RKNN/vimeo_triplet/sequences/00001"
calibration_data_vimeo = []

subdirs = sorted(glob(os.path.join(dataset_root, "*")))[:32]  # å–å‰ 32 ç»„æ ¡å‡†æ•°æ®

for subdir in subdirs:
    img1_path = os.path.join(subdir, "im1.png")  # ç¬¬ä¸€å¸§
    img2_path = os.path.join(subdir, "im2.png")  # ä¸­é—´å¸§ï¼ˆæ’å€¼ç›®æ ‡ï¼Œä¸ç”¨äºè¾“å…¥ï¼‰
    img3_path = os.path.join(subdir, "im3.png")  # ç¬¬ä¸‰å¸§
    # è¯»å–å›¾åƒ
    img1 = cv2.imread(img1_path)
    img3 = cv2.imread(img3_path)
    if img1 is None or img3 is None:
        print(f"âŒ è·³è¿‡æ— æ•ˆæ–‡ä»¶: {img1_path} æˆ– {img3_path}")
        continue  # é‡åˆ°ç©ºæ–‡ä»¶ï¼Œè·³è¿‡
    # BGR è½¬ RGB
    img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)
    img3 = cv2.cvtColor(img3, cv2.COLOR_BGR2RGB)
    # è°ƒæ•´å¤§å°ä¸º 256x256ï¼ˆå¯ä¿®æ”¹ï¼‰
    img1 = cv2.resize(img1, (256, 256), interpolation=cv2.INTER_AREA)
    img3 = cv2.resize(img3, (256, 256), interpolation=cv2.INTER_AREA)
    # è½¬æ¢ä¸º float32 å¹¶å½’ä¸€åŒ–åˆ° [0,1]
    img1 = torch.tensor(img1, dtype=torch.float32,requires_grad=False) / 255.0
    img3 = torch.tensor(img3, dtype=torch.float32,requires_grad=False) / 255.0
    # HWC è½¬ CHW
    img1 = img1.permute(2, 0, 1)  # (H, W, C) â†’ (C, H, W)
    img3 = img3.permute(2, 0, 1)  # (H, W, C) â†’ (C, H, W)
    # æ‹¼æ¥æˆ (6, 256, 256) ä½œä¸ºè¾“å…¥
    img_pair = torch.cat((img1, img3), dim=0)  # (6, 256, 256)
    calibration_data_vimeo.append(img_pair)


TRT_LOGGER = trt.Logger(trt.Logger.WARNING)


class Calibrator(trt.IInt8EntropyCalibrator2):
    def __init__(self,calibration_data_vimeo,batch_size=1,cache_file = "Calibrator.cache"):
        trt.IInt8EntropyCalibrator2.__init__(self)
        self.batch_size = batch_size
        self.calibration_data = calibration_data_vimeo
        self.cache_file = cache_file
        self.index = 0
        self.device_input = cuda.mem_alloc(batch_size * 6 * 256 * 256 * 4 * 32)

    def get_batch_size(self):
        return self.batch_size

    def get_batch(self, names):
        """
        è·å–ä¸€ä¸ª batch æ•°æ®ï¼Œå¹¶æ‹·è´åˆ° GPU
        """
        try:
            # å–å‡º batch_size ä¸ªæ ·æœ¬
            batch = self.calibration_data[self.index: self.index + self.batch_size]
            self.index += self.batch_size  # æ›´æ–°ç´¢å¼•
            if len(batch) == 0:
                return None  # æ²¡æœ‰æ•°æ®æ—¶è¿”å› Noneï¼Œé€šçŸ¥ TensorRT ç»“æŸæ ¡å‡†
            # å°† batch æ•°æ®è½¬æ¢ä¸º NumPy å¹¶æ‹·è´åˆ° GPU
            batch_np = torch.stack(batch).numpy().astype(np.float32)  # è½¬æ¢ä¸º NumPy (N, C, H, W)  torch.stackæ²¿æŸä¸ªç»´åº¦è¿æ¥å¤šä¸ªå¼ é‡çš„æ“ä½œ
            print(f"batch_np shape: {batch_np.shape}, dtype: {batch_np.dtype}, size: {batch_np.nbytes}")
            # ğŸš¨ ç¡®ä¿ batch_np æ˜¯ float32
            assert batch_np.dtype == np.float32, "âŒ batch_np éœ€è¦æ˜¯ float32!"

            batch_np = np.ascontiguousarray(batch_np, dtype=np.float32)     #ç¡®ä¿æ•°æ®æ˜¯è¿ç»­çš„
            ##ç¡®ä¿self.device_inputå·²ç»åˆ†é…
            if self.device_input is None:
                print("âŒ self.device_input is None, re-allocating memory!")
                self.device_input = cuda.mem_alloc(self.batch_size * 6 * 256 * 256 * 4 * 32)

            # ğŸš¨ ç¡®ä¿ batch_np å†…å­˜ä¸è¶…è¿‡åˆ†é…çš„ GPU å†…å­˜
            assert batch_np.nbytes <= self.batch_size * 6 * 256 * 256 * 4* 32, "âŒ batch_np è¶…å‡ºå·²åˆ†é…çš„ GPU å†…å­˜!"

            cuda.memcpy_htod(self.device_input, batch_np)  # æ‹·è´åˆ° GPU
            cuda.Context.synchronize()#ç¡®ä¿æ•°æ®åŒæ­¥
            return [int(self.device_input)]  # è¿”å› GPU å†…å­˜åœ°å€

        except Exception as e:
            print(f"[ERROR] get_batch å‘ç”Ÿå¼‚å¸¸: {e}")
            return None

    def read_calibration_cache(self):
        # å¦‚æœæ ¡å‡†è¡¨æ–‡ä»¶å­˜åœ¨åˆ™ç›´æ¥ä»å…¶ä¸­è¯»å–æ ¡å‡†è¡¨
        if os.path.exists(self.cache_file):
            with open(self.cache_file, "rb") as f:
                return f.read()

    def write_calibration_cache(self, cache):
        # å¦‚æœè¿›è¡Œäº†æ ¡å‡†ï¼Œåˆ™æŠŠæ ¡å‡†è¡¨å†™å…¥æ–‡ä»¶ä¸­ä»¥ä¾¿ä¸‹æ¬¡ä½¿ç”¨
        with open(self.cache_file, "wb") as f:
            f.write(cache)
            f.flush()


def build_engine(onnx_file_path, engine_file_path, mode="INT8", calibration_data=calibration_data_vimeo):
    builder = trt.Builder(TRT_LOGGER)#æ„å»ºå¼•æ“
    network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))    #builder.create_networkåˆ›å»ºè®¡ç®—å›¾ï¼ŒEXPLICITâ€”â€”BATCHå¼€å¯Batchæ¨¡å¼
    config = builder.create_builder_config() #builder.create_builder_configåˆ›å»ºé…ç½®ï¼Œç”¨äºè®¾ç½®FP16å’ŒINT8é‡åŒ–ç­‰ä¼˜åŒ–ç­–ç•¥
    parser = trt.OnnxParser(network, TRT_LOGGER)#trt.OnnxParserè§£æONNXï¼Œå°†onnxè®¡ç®—å›¾è½¬æ¢ä¸ºtensorrtæ ¼å¼

    # è¯»å– ONNX æ¨¡å‹
    assert os.path.exists(onnx_file_path), f"ONNX file {onnx_file_path} not found!"
    with open(onnx_file_path, "rb") as model:## æ‰“å¼€ONNXæ–‡ä»¶å¹¶è§£æä¸ºTensorRTè®¡ç®—å›¾
        if not parser.parse(model.read()):
            print("Failed to parse the ONNX file.")
            for error in range(parser.num_errors):
                print(parser.get_error(error))
            return None

    print(f"Building TensorRT engine from {onnx_file_path}, this may take a while...")

    # è®¾ç½® TensorRT çš„å·¥ä½œç©ºé—´å†…å­˜
    config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 4 << 30)  # 4GB å·¥ä½œç©ºé—´

    # é…ç½® INT8 é‡åŒ–
    if mode == "INT8":
        config.set_flag(trt.BuilderFlag.INT8)       ##å¯ç”¨INT8
        assert calibration_data is not None, "Calibration data is required for INT8 mode!"
        calibrator = Calibrator(calibration_data)
        config.int8_calibrator = calibrator
        print("Using INT8 mode for optimization.")
    # # é…ç½® FP16 è®¡ç®—
    # elif mode == "FP16":    ##ä¸éœ€è¦æ ¡å‡†æ•°æ®
    #     config.set_flag(trt.BuilderFlag.FP16)
    #     print("Using FP16 mode for optimization.")

    # ç”Ÿæˆ TensorRT engine
    engine = builder.build_engine(network, config)#æ„å»ºTensorRT Engine
    if engine is None:
        print("Failed to create the engine.")
        return None

    # ä¿å­˜ TensorRT engine åˆ°æ–‡ä»¶
    with open(engine_file_path, "wb") as f:     #åºåˆ—åŒ–Engineï¼Œè½¬æ¢ä¸ºäºŒè¿›åˆ¶æ–‡ä»¶
        f.write(engine.serialize())

    print(f"TensorRT engine saved to {engine_file_path}")
    return engine

# ç¤ºä¾‹ä½¿ç”¨
onnx_path = r"/home/jason/RIFE_ONNX_TRT_RKNN/ECCV2022-RIFE/train_log/IFNet_fp32.onnx"
engine_path = r"/home/jason/RIFE_ONNX_TRT_RKNN/ECCV2022-RIFE/train_log/model_int8.trt"
build_engine(onnx_path, engine_path, mode="INT8", calibration_data=calibration_data_vimeo)